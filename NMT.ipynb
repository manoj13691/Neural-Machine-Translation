{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "colab_type": "code",
    "id": "YPEz3w1I_5uE",
    "outputId": "eec80937-1f6b-4a55-db49-d374599ba652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1331672\n",
      "-rw-r--r-- 1 root root 644937323 Aug 22  2014 train.en\n",
      "-rw-r--r-- 1 root root 717610118 Aug 22  2014 train.de\n",
      "-rw-r--r-- 1 root root    403998 Jan 27  2015 vocab.50K.en\n",
      "-rw-r--r-- 1 root root    505304 Jan 27  2015 vocab.50K.de\n",
      "drwxr-xr-x 2 root root      4096 Sep 10 17:46 sample_data\n",
      "-rw-r--r-- 1 root root     14267 Sep 11 23:38 wget-log\n",
      "-rw-r--r-- 1 root root     12667 Sep 11 23:38 wget-log.1\n",
      "-rw-r--r-- 1 root root       989 Sep 11 23:39 wget-log.2\n",
      "-rw-r--r-- 1 root root       989 Sep 11 23:39 wget-log.3\n",
      "-rw-r--r-- 1 root root     40231 Sep 11 23:39 en-embeddings.npy\n",
      "-rw-r--r-- 1 root root       677 Sep 11 23:39 wget-log.4\n",
      "-rw-r--r-- 1 root root       677 Sep 11 23:39 wget-log.5\n",
      "-rw-r--r-- 1 root root     40225 Sep 11 23:39 de-embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de\n",
    "\n",
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en\n",
    "\n",
    "!ls -lrt\n",
    "\n",
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.en\n",
    "\n",
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.de\n",
    "\n",
    "!wget https://github.com/thushv89/exercises_thushv_dot_com/blob/master/en-embeddings.npy\n",
    "\n",
    "!wget https://github.com/thushv89/exercises_thushv_dot_com/blob/master/de-embeddings.npy\n",
    "\n",
    "!ls -lrt\n",
    "\n",
    "#### All the necessary files have been downloaded.\n",
    "#### Now we will begin building the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hide all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "26zowsLBARNx"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Seq2Seq Items\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "16nutZFeCBII"
   },
   "outputs": [],
   "source": [
    "# Building the dictionary required for word lookup.\n",
    "# To dictionaries built - one for word to key and other for key to word. \n",
    "# The dictionary is built for both source and target language\n",
    "\n",
    "src_dictionary = dict()\n",
    "with open(\"vocab.50K.de\", encoding = \"utf-8\") as f:\n",
    "  word_count = 0\n",
    "  for line in f:\n",
    "    src_dictionary[line.split(\"\\n\")[0]] = word_count \n",
    "    word_count +=1\n",
    "\n",
    "src_reverse_dictionary = dict(zip(src_dictionary.values(), src_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "Mf8Rg8EDNiYP",
    "outputId": "c5a61df5-d68f-431c-8b64-678249275eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "das\n",
      "16\n",
      "Vocabulary:50000\n"
     ]
    }
   ],
   "source": [
    "# Check whether the above code worked right. \n",
    "# The word 'das' is mapped to 16\n",
    "\n",
    "print(src_reverse_dictionary[16])\n",
    "print(src_dictionary['das'])\n",
    "print(\"Vocabulary:\"+ str(len(src_dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gphF9bdJOMUu"
   },
   "outputs": [],
   "source": [
    "tgt_dictionary = dict()\n",
    "with open(\"vocab.50K.en\", encoding = \"utf-8\") as f:\n",
    "  word_count = 0\n",
    "  for line in f:\n",
    "    tgt_dictionary[line.split(\"\\n\")[0]] = word_count \n",
    "    word_count +=1\n",
    "\n",
    "tgt_reverse_dictionary = dict(zip(tgt_dictionary.values(), tgt_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "aPLAvQu9PHS7",
    "outputId": "10aa1dd1-0957-4093-d8ae-80d00a112375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you\n",
      "28\n",
      "Vocabulary:50000\n"
     ]
    }
   ],
   "source": [
    "# Check whether the above code worked right. \n",
    "# The word 'you' is mapped to 28\n",
    "\n",
    "print(tgt_reverse_dictionary[28])\n",
    "print(tgt_dictionary['you'])\n",
    "print(\"Vocabulary:\"+ str(len(tgt_dictionary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9uZkxI-6RfvM"
   },
   "source": [
    "#### Now, lets load the german (source) and the english (target) sentences separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "SrcMGAAzRh4m",
    "outputId": "83bda771-55fb-4fbf-bf20-86b93c93c1cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "source_sent = []\n",
    "no_sentences_to_be_read = 60000\n",
    "\n",
    "with open(\"train.de\", encoding = \"utf-8\") as f:\n",
    "    count = 1\n",
    "    for line in f:\n",
    "        if count > no_sentences_to_be_read:\n",
    "            break\n",
    "        line = line.split(\"\\n\")[0]\n",
    "        source_sent.append(line)\n",
    "        count +=1\n",
    "\n",
    "print(len(source_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kpxRLJbWTQXK"
   },
   "outputs": [],
   "source": [
    "# The first 51 sentences were English for some reason\n",
    "source_sent = source_sent[52:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "rw9oipSod-bz",
    "outputId": "6f328940-08f6-4795-fcc3-337114f34847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "target_sent = []\n",
    "\n",
    "with open(\"train.en\", encoding = \"utf-8\") as f:\n",
    "    count = 1\n",
    "    for line in f:\n",
    "        if count > no_sentences_to_be_read:\n",
    "            break\n",
    "        line = line.split(\"\\n\")[0]\n",
    "        target_sent.append(line)\n",
    "        count +=1\n",
    "\n",
    "print(len(target_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2E6kXvE5etUy"
   },
   "outputs": [],
   "source": [
    "#The number of sentences must be same for both source and target\n",
    "target_sent = target_sent[52:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ujacwGDuev1_"
   },
   "outputs": [],
   "source": [
    "assert(len(source_sent) == len(target_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "e3N42HVXfxbc",
    "outputId": "998c44c2-92b3-4f88-9a44-5ea926c83085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (German): Sie werden überrascht sein , wie einfach sich mit Quark das volle Potenzial Ihrer Design ##AT##-##AT## Software erschließen lässt .\n",
      "Target (English): You ’ ll be surprised how easy Quark has made it to unlock the full potential of all your design software .\n",
      "\n",
      "\n",
      "Source (German): Auch die französische Bahn führt in Zusammenarbeit mit dem Conseil Régional Busverkehre insbesondere Richtung Aix und darüber hinaus in die Alpen durch .\n",
      "Target (English): Marseille has a big harbour . There are direct ferry routes from Marseille to Ajaccio , Bastia , Porto Torres , Porto Vecchio and Propriano .\n",
      "\n",
      "\n",
      "Source (German): Einzel Betten die zusammen gestellt waren .\n",
      "Target (English): Staff was slow to respond to a broken shower , another room &apos;s shower was used for 3 days .\n",
      "\n",
      "\n",
      "Source (German): Denn im Anschluss an diese psychologische Definition betont der Autor Weinberg unmissverständlich , dass ihn genau diese klinischen Aspekte des Leidens an und des Heilens von seelischen Verletzungen bei seiner Beschäftigung mit dem Trauma ausdrücklich nicht interessieren ( Weinberg , 1999 , S.\n",
      "Target (English): Never having read something like this , Dr Goodheart is interested and continues reading : &quot; And for this very reason , the trauma must remain inaccessible &#91; to memory &#93; &quot; . Weinberg seems to have no problem with this , since &quot; trauma &quot; means &quot; truth &quot; , or more precisely , &quot; the inaccessible truth of remembering &quot; ( 77 ) .\n",
      "\n",
      "\n",
      "Source (German): Durch die Vermeidung der sonst üblichen Zwischenschritte ist das Signal ##AT##-##AT## Rausch ##AT##-##AT## Verhältnis und damit die Kontrastauflösung um ein Vielfaches besser als bei analogen oder Videobildketten .\n",
      "Target (English): Unlike conventional image intensifier chains , digital image chains provide better signal ##AT##-##AT## noise ratios and higher image contrast while eliminating image artefacts and distortions .\n",
      "\n",
      "\n",
      "Source (German): Ziel der ADL ist die Integration der Arbeit verschiedener Konsortien , die sich des Themas E ##AT##-##AT## Learning bzw. IDL ( Interactive Distance Learning ) angenommen haben , um einen allgemein anerkannten Standard zu definieren .\n",
      "Target (English): One main objective of this initiative is the integration of the work of several consortia , which have dealt with E ##AT##-##AT## Learning respectively IDL ( Interactive Distance Learning ) , in order to define a generally accepted standard . The result of this effort is called SCORM .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets print some samples of both the source and target languages\n",
    "for i in range(0, len(source_sent), 10000):\n",
    "    print(\"Source (German):\", source_sent[i])\n",
    "    print(\"Target (English):\", target_sent[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2tpSKcXXgg67"
   },
   "outputs": [],
   "source": [
    "# Split the sentence into space separated tokens. \n",
    "# If tokens are not present in the vocabulary, then we add the <unk> unknown\n",
    "# token.\n",
    "def split_to_token(sentence, is_source):\n",
    "    sentence = sentence.replace(\",\",\" ,\")\n",
    "    sentence = sentence.replace(\".\",\" .\")\n",
    "    sentence = sentence.replace(\"\\n\",\" \")\n",
    "  \n",
    "    tokens = sentence.split(\" \")\n",
    "  \n",
    "    for i in range(len(tokens)):\n",
    "        if is_source:\n",
    "            if tokens[i] not in src_dictionary:\n",
    "                tokens[i] = '<unk>'\n",
    "    \n",
    "        else:\n",
    "            if tokens[i] not in tgt_dictionary:\n",
    "                tokens[i] = '<unk>'\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "colab_type": "code",
    "id": "up9CpTwMzfWv",
    "outputId": "f6ed1b53-ca74-4767-fb54-5b5e89df41d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length in source (German): 25.47135851070928\n",
      "Mean length in target: (English) 27.65571828918396\n",
      "\n",
      "\n",
      "Max length in source (German): 118\n",
      "Max length in target: (English) 119\n"
     ]
    }
   ],
   "source": [
    "# Finding the mean, max length of the full data\n",
    "source_len = []\n",
    "target_len = []\n",
    "\n",
    "# Number of source and target samples are same. \n",
    "for i in range(len(source_sent)):\n",
    "    source_len.append(len(split_to_token(source_sent[i], True)))\n",
    "    target_len.append(len(split_to_token(target_sent[i], False)))\n",
    "\n",
    "print(\"Mean length in source (German):\" , np.mean(source_len))\n",
    "print(\"Mean length in target: (English)\" , np.mean(target_len))\n",
    "print(\"\\n\")\n",
    "print(\"Max length in source (German):\" , np.max(source_len))\n",
    "print(\"Max length in target: (English)\" , np.max(target_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "I5fNShv4TUAZ",
    "outputId": "a390eb91-5b12-426f-8b03-170700584307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of source sentences: 59948\n",
      "Total number of target sentences: 59948\n",
      "Length of each source sentence: 40\n",
      "Length of each target sentence: 50\n"
     ]
    }
   ],
   "source": [
    "train_inputs = []\n",
    "train_outputs = []\n",
    "train_inp_lengths = []\n",
    "train_out_lengths = []\n",
    "\n",
    "max_src_length = 40\n",
    "max_tgt_length = 50\n",
    "\n",
    "for i in range(len(source_sent)):\n",
    "    src_tokens = split_to_token(source_sent[i], True)\n",
    "    target_tokens = split_to_token(target_sent[i], False)\n",
    "  \n",
    "    src_sentence_numbers = []\n",
    "    for token in src_tokens:\n",
    "        src_sentence_numbers.append(src_dictionary[token])\n",
    "    \n",
    "    \n",
    "    target_sentence_numbers = []\n",
    "    # Add a token which indicates the end of source and begining of the target.\n",
    "    target_sentence_numbers.append(tgt_dictionary['</s>'])\n",
    "  \n",
    "    for token in target_tokens:\n",
    "        target_sentence_numbers.append(tgt_dictionary[token])\n",
    "   \n",
    "    # Reverse the source sentence list for better performance/translation. \n",
    "    # This fact is based out of a paper for NMT.\n",
    "  \n",
    "    src_sentence_numbers = src_sentence_numbers[::-1]\n",
    "\n",
    "    # Add the start symbol at the start of source.\n",
    "    src_sentence_numbers.insert(0, src_dictionary['<s>'])\n",
    "    train_inp_lengths.append(min(len(src_sentence_numbers)+1,max_src_length))\n",
    "    # Make sure that both the source and target have same length.\n",
    "\n",
    "\n",
    "    if len(src_sentence_numbers) < max_src_length:\n",
    "        src_sentence_numbers.extend([ src_dictionary['</s>'] for i in range( max_src_length - len(src_sentence_numbers) )])\n",
    "\n",
    "    elif len(src_sentence_numbers) > max_src_length:\n",
    "        src_sentence_numbers = src_sentence_numbers[:max_src_length]\n",
    "\n",
    "\n",
    "    if len(target_sentence_numbers) < max_tgt_length:\n",
    "        target_sentence_numbers.extend([ tgt_dictionary['</s>'] for i in range( max_tgt_length - len(target_sentence_numbers) )])\n",
    "\n",
    "    elif len(target_sentence_numbers) > max_tgt_length:\n",
    "        target_sentence_numbers = target_sentence_numbers[:max_tgt_length]\n",
    "\n",
    "\n",
    "    if len(src_sentence_numbers) == max_src_length and len(target_sentence_numbers) == max_tgt_length:\n",
    "        train_inputs.append(src_sentence_numbers)\n",
    "        train_outputs.append(target_sentence_numbers)\n",
    "\n",
    "train_inp_lengths = np.array(train_inp_lengths, dtype=np.int32)\n",
    "print(\"Total number of source sentences:\", len(train_inputs))\n",
    "print(\"Total number of target sentences:\", len(train_outputs))\n",
    "print(\"Length of each source sentence:\", len(train_inputs[0]))\n",
    "print(\"Length of each target sentence:\", len(train_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DhPwJY2pAX9C"
   },
   "outputs": [],
   "source": [
    "# Convert the source and target to numpy values\n",
    "train_inputs = np.array(train_inputs, dtype=np.int32)\n",
    "train_outputs = np.array(train_outputs, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "iUGTJNGyB2yE",
    "outputId": "88cb3909-a885-452f-f808-65caff2bbd9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '.', '<unk>', 'lässt', 'erschließen', 'Software', '##AT##-##AT##', 'Design', 'Ihrer', 'Potenzial', 'volle', 'das', 'Quark', 'mit', 'sich', 'einfach', 'wie', ',', '<unk>', 'sein', 'überrascht', 'werden', 'Sie', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', 'You', '’', 'll', 'be', 'surprised', 'how', 'easy', 'Quark', 'has', 'made', 'it', 'to', 'unlock', 'the', 'full', 'potential', 'of', 'all', 'your', 'design', 'software', '<unk>', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['<s>', '.', '<unk>', 'anreisen', '<unk>', 'dem', 'oder', 'Zug', 'dem', 'mit', 'auch', 'man', 'kann', 'Natürlich', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', 'To', 'get', 'to', 'Barcelona', 'by', 'car', 'from', 'Madrid', '<unk>', ',', 'you', 'should', 'take', 'the', 'Nacional', 'II', 'motorway', '<unk>', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# See some sample source and target sentences.\n",
    "# The source sentence is reversed\n",
    "print([ src_reverse_dictionary[i] for i in train_inputs[0] ])\n",
    "print([ tgt_reverse_dictionary[i] for i in train_outputs[0] ])\n",
    "\n",
    "print([ src_reverse_dictionary[i] for i in train_inputs[100] ])\n",
    "print([ tgt_reverse_dictionary[i] for i in train_outputs[100] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "d5iRtVr4CWHK"
   },
   "outputs": [],
   "source": [
    "class DataGeneratorMT(object):\n",
    "\n",
    "    def __init__(self,batch_size,num_unroll,is_source):\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "\n",
    "        self._sent_ids = None\n",
    "\n",
    "        self._is_source = is_source\n",
    "\n",
    "\n",
    "    def print(self):\n",
    "        print(self._cursor)\n",
    "\n",
    "\n",
    "    def next_batch(self, sent_ids):\n",
    "        if self._is_source:\n",
    "            max_sent_length = max_src_length\n",
    "        else:\n",
    "            max_sent_length = max_tgt_length\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            sent_id = sent_ids[b]\n",
    "\n",
    "            if self._is_source:\n",
    "                sent_text = train_inputs[sent_id]\n",
    "                batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b] = sent_text[self._cursor[b] + 1]\n",
    "                \n",
    "            else:\n",
    "                sent_text = train_outputs[sent_id]\n",
    "                batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b] = sent_text[self._cursor[b] + 1]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b] + 1)%(max_sent_length - 1)\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "\n",
    "    def unroll_batches(self, sent_ids):\n",
    "        if sent_ids is not None:\n",
    "            self._sent_ids = sent_ids\n",
    "\n",
    "        unroll_data, unroll_labels = [], []\n",
    "        inp_lengths = None\n",
    "        for i in range(self._num_unroll):\n",
    "            data, labels = self.next_batch(self._sent_ids)\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "            inp_lengths = train_inp_lengths[sent_ids]\n",
    "\n",
    "        return unroll_data, unroll_labels, self._sent_ids, inp_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1253
    },
    "colab_type": "code",
    "id": "5ndWtZ4FONzj",
    "outputId": "4277f46b-e8f5-43c1-9be7-3aa81be59aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data - German\n",
      "['<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['.', '.', '.', '.', '.']\n",
      "\n",
      "\n",
      "['.', '.', '.', '.', '.']\n",
      "['<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "\n",
      "\n",
      "['<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "['lässt', 'bietet', 'nutzen', 'werden', 'müssen']\n",
      "\n",
      "\n",
      "['lässt', 'bietet', 'nutzen', 'werden', 'müssen']\n",
      "['erschließen', 'Dateiformat', 'optimal', 'ausgewählt', 'zu']\n",
      "\n",
      "\n",
      "['erschließen', 'Dateiformat', 'optimal', 'ausgewählt', 'zu']\n",
      "['Software', '##AT##-##AT##', 'Bilder', 'Verwendungszweck', 'generieren']\n",
      "\n",
      "\n",
      "['Software', '##AT##-##AT##', 'Bilder', 'Verwendungszweck', 'generieren']\n",
      "['##AT##-##AT##', 'PSD', 'Ihre', 'nach', 'Datei']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ob = DataGeneratorMT(5, 40, True)\n",
    "data, label, _, _ = ob.unroll_batches([0,1,2,3,4])\n",
    "#print(data)\n",
    "#print(label)\n",
    "\n",
    "count = 0\n",
    "print('Source data - German')\n",
    "for dta, lbl in zip(data,label):\n",
    "    if count > 5:\n",
    "        break\n",
    "    print([src_reverse_dictionary[w] for w in dta.tolist()])\n",
    "    print([src_reverse_dictionary[w] for w in lbl.tolist()])\n",
    "    print(\"\\n\")\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The above output looks fine. \n",
    "#### In seq2seq model, if we have [1,2,3,4,5] as input, using batch size of 3, we can have [1,2,3] , [2,3,4], [3,4,5] as batches.\n",
    "#### Because of this, we have the code:\n",
    "##### batch_data[b] = sent_text[self._cursor[b]]\n",
    "##### batch_labels[b] = sent_text[self._cursor[b] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target data - English\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['You', 'QuarkXPress', 'In', 'For', 'If']\n",
      "\n",
      "\n",
      "['You', 'QuarkXPress', 'In', 'For', 'If']\n",
      "['’', '8', 'this', 'example', 'you']\n",
      "\n",
      "\n",
      "['’', '8', 'this', 'example', 'you']\n",
      "['ll', 'is', 'section', '<unk>', 'use']\n",
      "\n",
      "\n",
      "['ll', 'is', 'section', '<unk>', 'use']\n",
      "['be', 'considered', 'we', ',', 'PSD']\n",
      "\n",
      "\n",
      "['be', 'considered', 'we', ',', 'PSD']\n",
      "['surprised', 'by', '’', 'you', '<unk>']\n",
      "\n",
      "\n",
      "['surprised', 'by', '’', 'you', '<unk>']\n",
      "['how', 'many', 'll', 'may', ',']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ob = DataGeneratorMT(5, 40, False)\n",
    "data, label, _, _ = ob.unroll_batches([0,1,2,3,4])\n",
    "#print(data)\n",
    "#print(label)\n",
    "\n",
    "count = 0\n",
    "print('Target data - English')\n",
    "for dta, lbl in zip(data,label):\n",
    "    if count > 5:\n",
    "        break\n",
    "    print([tgt_reverse_dictionary[w] for w in dta.tolist()])\n",
    "    print([tgt_reverse_dictionary[w] for w in lbl.tolist()])\n",
    "    print(\"\\n\")\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Load pre-trained word embeddings for german and english\n",
    "encoder_emb_layer = tf.convert_to_tensor(np.load('de-embeddings.npy'))\n",
    "decoder_emb_layer = tf.convert_to_tensor(np.load('en-embeddings.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (German) Embeddings\n",
      "[ 0.02807283  0.08129065  0.04325406  0.03861211 -0.04374592  0.08648341\n",
      " -0.12594177  0.05204354  0.02891895 -0.00274239  0.0464763  -0.0177199\n",
      "  0.0113791   0.10005165 -0.13852786  0.07391532  0.14600192 -0.07613634\n",
      "  0.0165039   0.09500151 -0.09135051  0.06103227 -0.09518221 -0.00840024\n",
      "  0.1021672  -0.09210443  0.05864106  0.02367448 -0.12617454  0.03162083\n",
      " -0.00553827 -0.06233861 -0.09098011  0.04980104 -0.08403688 -0.02544336\n",
      " -0.03200009 -0.36211336  0.04187389  0.13499737  0.01335561  0.05164875\n",
      "  0.07950916  0.04037105  0.07873604  0.01508441 -0.01101452 -0.02970273\n",
      " -0.11738252  0.02539947 -0.10869873 -0.04156078  0.0270797   0.09760202\n",
      "  0.01272728  0.12135591 -0.09459837 -0.08765817  0.04319254 -0.04632879\n",
      "  0.03997531 -0.09053234 -0.05756423 -0.20108807 -0.04703222 -0.05928725\n",
      " -0.00646044  0.14901383  0.05235337 -0.00089508 -0.05842103 -0.01100476\n",
      "  0.11242474 -0.09033585 -0.04254638 -0.15847538 -0.12196966 -0.10012189\n",
      "  0.04274154 -0.10345502  0.00574398  0.07736626  0.11999135 -0.08652569\n",
      " -0.03872042  0.10062841  0.09217047  0.03666938 -0.04574378  0.06360335\n",
      " -0.08939499  0.10636167  0.01082712 -0.11277407  0.02705009 -0.02527077\n",
      " -0.24460979 -0.05275669  0.05250627 -0.04010779 -0.04981223 -0.15472683\n",
      "  0.15857296 -0.02322546  0.05232251 -0.04510524  0.15454935 -0.02650067\n",
      " -0.0042775   0.10024884 -0.03968636  0.06309864  0.09832346 -0.02791295\n",
      " -0.14186773 -0.02850333  0.05871908  0.08428557  0.04390426 -0.07982356\n",
      "  0.08855276 -0.04197617  0.07706168 -0.04147149  0.18912494  0.05615143\n",
      "  0.1390056   0.08622716]\n",
      "Number of Embeddings: 50000\n",
      "Source Embedding Size: 128\n",
      "Target Embedding Size: 128\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()  \n",
    "print(\"Source (German) Embeddings\")\n",
    "print(encoder_emb_layer.eval()[0])\n",
    "#print(\"Target (English) Embeddings\")\n",
    "#print(decoder_emb_layer.eval()[0])\n",
    "\n",
    "# Check if the length of source and target embeddings are same.\n",
    "assert(len(encoder_emb_layer.eval()) == len(decoder_emb_layer.eval()))\n",
    "assert(len(encoder_emb_layer.eval()[0]) == len(decoder_emb_layer.eval()[0]))\n",
    "\n",
    "print(\"Number of Embeddings:\", len(encoder_emb_layer.eval()))\n",
    "print(\"Source Embedding Size:\",len(encoder_emb_layer.eval()[0]))\n",
    "print(\"Target Embedding Size:\",len(decoder_emb_layer.eval()[0]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of encoder train inputs: 40 x 16\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "enc_train_inputs = []\n",
    "dec_train_inputs = []\n",
    "dec_train_labels=[]\n",
    "dec_label_masks = []\n",
    "\n",
    "print(\"Dimension of encoder train inputs:\", max_src_length, \"x\",batch_size)\n",
    "for i in range(max_src_length):\n",
    "    enc_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size],name='enc_train_inputs_%d'%i))\n",
    "\n",
    "print(len(enc_train_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of decoder train inputs: 50 x 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of decoder train inputs:\", max_tgt_length, \"x\",batch_size)\n",
    "for i in range(max_tgt_length):\n",
    "    dec_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size], name='dec_train_inputs_%d'%i))\n",
    "    dec_train_labels.append(tf.placeholder(tf.int32, shape=[batch_size], name='dec_train_labels_%d'%i))\n",
    "    dec_label_masks.append(tf.placeholder(tf.float32, shape=[batch_size], name='dec_label_masks_%d'%i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02807283  0.08129065  0.04325406  0.03861211 -0.04374592  0.08648341\n",
      " -0.12594177  0.05204354  0.02891895 -0.00274239  0.0464763  -0.0177199\n",
      "  0.0113791   0.10005165 -0.13852786  0.07391532  0.14600192 -0.07613634\n",
      "  0.0165039   0.09500151 -0.09135051  0.06103227 -0.09518221 -0.00840024\n",
      "  0.1021672  -0.09210443  0.05864106  0.02367448 -0.12617454  0.03162083\n",
      " -0.00553827 -0.06233861 -0.09098011  0.04980104 -0.08403688 -0.02544336\n",
      " -0.03200009 -0.36211336  0.04187389  0.13499737  0.01335561  0.05164875\n",
      "  0.07950916  0.04037105  0.07873604  0.01508441 -0.01101452 -0.02970273\n",
      " -0.11738252  0.02539947 -0.10869873 -0.04156078  0.0270797   0.09760202\n",
      "  0.01272728  0.12135591 -0.09459837 -0.08765817  0.04319254 -0.04632879\n",
      "  0.03997531 -0.09053234 -0.05756423 -0.20108807 -0.04703222 -0.05928725\n",
      " -0.00646044  0.14901383  0.05235337 -0.00089508 -0.05842103 -0.01100476\n",
      "  0.11242474 -0.09033585 -0.04254638 -0.15847538 -0.12196966 -0.10012189\n",
      "  0.04274154 -0.10345502  0.00574398  0.07736626  0.11999135 -0.08652569\n",
      " -0.03872042  0.10062841  0.09217047  0.03666938 -0.04574378  0.06360335\n",
      " -0.08939499  0.10636167  0.01082712 -0.11277407  0.02705009 -0.02527077\n",
      " -0.24460979 -0.05275669  0.05250627 -0.04010779 -0.04981223 -0.15472683\n",
      "  0.15857296 -0.02322546  0.05232251 -0.04510524  0.15454935 -0.02650067\n",
      " -0.0042775   0.10024884 -0.03968636  0.06309864  0.09832346 -0.02791295\n",
      " -0.14186773 -0.02850333  0.05871908  0.08428557  0.04390426 -0.07982356\n",
      "  0.08855276 -0.04197617  0.07706168 -0.04147149  0.18912494  0.05615143\n",
      "  0.1390056   0.08622716]\n"
     ]
    }
   ],
   "source": [
    "# Sample code to read and display an embedding\n",
    "sample_embedding = tf.nn.embedding_lookup(encoder_emb_layer, 0)\n",
    "sess = tf.InteractiveSession()\n",
    "print(sample_embedding.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup_1:0\", shape=(16, 128), dtype=float32)\n",
      "Tensor(\"stack:0\", shape=(40, 16, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Each word_int is of length 16. 16 id's are passed to embedding_lookup to get their 128 dimension embedding.\n",
    "# So , the embedding_lookup returns 16x128 vector. \n",
    "# This process to repeated 40 times - which is the length of enc_train_inputs\n",
    "encoder_emb_inp = [tf.nn.embedding_lookup(encoder_emb_layer, word_int) for word_int in enc_train_inputs]\n",
    "print(encoder_emb_inp[0])\n",
    "encoder_emb_inp = tf.stack(encoder_emb_inp)\n",
    "# All the 40 elements are stacked to form a single tensor.\n",
    "print(encoder_emb_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup_41:0\", shape=(16, 128), dtype=float32)\n",
      "Tensor(\"stack_1:0\", shape=(50, 16, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoder_emb_inp = [tf.nn.embedding_lookup(decoder_emb_layer, word_int) for word_int in dec_train_inputs]\n",
    "print(decoder_emb_inp[0])\n",
    "decoder_emb_inp = tf.stack(decoder_emb_inp)\n",
    "print(decoder_emb_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"train_input_lengths:0\", shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "enc_train_inp_lengths = tf.placeholder(tf.int32, shape=[batch_size],name='train_input_lengths')\n",
    "dec_train_inp_lengths = tf.placeholder(tf.int32, shape=[batch_size],name='train_output_lengths')\n",
    "print(enc_train_inp_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The input and output layers end here.\n",
    "#### Next we begin the encoder, decoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMStateTuple(c=<tf.Tensor 'BasicLSTMCellZeroState/zeros:0' shape=(16, 128) dtype=float32>, h=<tf.Tensor 'BasicLSTMCellZeroState/zeros_1:0' shape=(16, 128) dtype=float32>)\n",
      "\n",
      "\n",
      "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(16, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(16, 128) dtype=float32>)\n",
      "\n",
      "\n",
      "\n",
      "Tensor(\"rnn/TensorArrayStack/TensorArrayGatherV3:0\", shape=(40, 16, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_units = 128 \n",
    "# num_units  - Parameter of BasicLSTMCell - Number of hidden neurons in each cell\n",
    "# Ideally, its size should be the size of the embeddings.\n",
    "\n",
    "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=num_units)\n",
    "initial_state = encoder_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "print(initial_state)\n",
    "\n",
    "# Reference - https://stackoverflow.com/questions/41885519/tensorflow-dynamic-rnn-parameters-meaning\n",
    "\"\"\"\n",
    "cell -            Each cell of the sequential modelling. In our case, each cell is a Basic LSTM Cell with \n",
    "                  128 neurons.\n",
    "\n",
    "inputs -          Its the input for the entire sequence model.\n",
    "                  If we use, time_major = True then dimension of encoder_emb_inp should be \n",
    "                  (max_time, batch_size, input_size - each embedding size) = (40, 16, 128)\n",
    "        \n",
    "initial_state  -  The hidden states of all the LSTM cells for each sequence. Hidden states are initialized\n",
    "                  for each sequence i.e., no hidden state is passed between the sequences even if they are\n",
    "                  from the same batch.\n",
    "                 \n",
    "sequence_length - is a vector of size batch_size in which each element gives the length of each sequence in \n",
    "                  the batch. \n",
    "\n",
    "swap_memory     - If True, Tensors are swapped between CPU and GPU.\n",
    "\"\"\"\n",
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "    cell= encoder_cell, inputs = encoder_emb_inp, initial_state=initial_state,\n",
    "    sequence_length=enc_train_inp_lengths, \n",
    "    time_major=True, swap_memory=True)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(encoder_state)\n",
    "print(\"\\n\\n\")\n",
    "print(encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f50bc1775f8>\n",
      "<tensorflow.python.layers.core.Dense object at 0x7f50b56886d8>\n"
     ]
    }
   ],
   "source": [
    "decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "print(decoder_cell)\n",
    "vocab_size = 50000\n",
    "\n",
    "# Projection layer is used to get the output of every LSTM cell\n",
    "projection_layer = Dense(units=vocab_size, use_bias=True)\n",
    "print(projection_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n"
     ]
    }
   ],
   "source": [
    "seq_len_vector_for_helper = [max_tgt_length for _ in range(batch_size)]\n",
    "print(seq_len_vector_for_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(decoder_emb_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoder object at 0x7f50b5688940>\n"
     ]
    }
   ],
   "source": [
    "# TrainingHelper is used to feed the ground truth at every step instead of the decoded output \n",
    "# value from the previous step.\n",
    "# Reference- https://stackoverflow.com/questions/43826784/trouble-understanding-tf-contrib-seq2seq-traininghelper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_emb_inp, sequence_length= seq_len_vector_for_helper, \n",
    "                                           time_major= True)\n",
    "#Final encoder state becomes the first input for Decoder.\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection_layer)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "outputs, final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, output_time_major=True, swap_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = outputs.rnn_output\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=dec_train_labels, logits=logits)\n",
    "loss = tf.reduce_sum(cross_entropy * tf.stack(dec_label_masks))/(batch_size * max_tgt_length)\n",
    "train_prediction = outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Optimizer\n"
     ]
    }
   ],
   "source": [
    "print('Defining Optimizer')\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    0.01, global_step, decay_steps=10, decay_rate=0.9, staircase=True)\n",
    "\n",
    "with tf.variable_scope('Adam'):\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "#Reference - https://www.tensorflow.org/api_docs/python/tf/train/Optimizer    \n",
    "    \n",
    "# When we zip, all the gradients are grouped in one tuple\n",
    "# and all the variables are grouped in one.\n",
    "# adam_gradients = (v1_grad, v2_grad, v3_grad...) \n",
    "# variable = (v1, v2, v3...)\n",
    "# We do this to apply gradient clipping on all the gradients. \n",
    "adam_gradients, variable = zip(*adam_optimizer.compute_gradients(loss))\n",
    "adam_gradients, _ = tf.clip_by_global_norm(adam_gradients, 25.0)\n",
    "\n",
    "#We convert back to the original form of [(grad1, variable1), (grad2, v)....] to apply gradients\n",
    "adam_optimize = adam_optimizer.apply_gradients(zip(adam_gradients, variable))\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[18112  5599 58807  6205  5813 32318 20018 17348 17710 44079 37479 48564\n",
      " 19415 34191 43499 54809]\n",
      "[40 30 18 13 39 30 11 12 26 33 36 14 20 12 17 17]\n"
     ]
    }
   ],
   "source": [
    "enc_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=max_src_length,is_source=True)\n",
    "dec_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=max_tgt_length,is_source=False)\n",
    "\n",
    "enc_data_generator.print()\n",
    "\n",
    "sent_ids = np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
    "print(sent_ids)\n",
    "# ====================== ENCODER DATA COLLECTION ================================================\n",
    "\n",
    "eu_data, eu_labels, _, eu_lengths = enc_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "feed_dict[enc_train_inp_lengths] = eu_lengths\n",
    "for ui,(dat,lbl) in enumerate(zip(eu_data,eu_labels)):     \n",
    "    #print(ui)\n",
    "    feed_dict[enc_train_inputs[ui]] = dat                \n",
    "\n",
    "# ====================== DECODER DATA COLLECTION ===========================\n",
    "\n",
    "du_data, du_labels, _, du_lengths = dec_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "print(du_lengths)\n",
    "\n",
    "feed_dict[dec_train_inp_lengths] = du_lengths\n",
    "for ui,(dat,lbl) in enumerate(zip(du_data,du_labels)):            \n",
    "    feed_dict[dec_train_inputs[ui]] = dat\n",
    "    feed_dict[dec_train_labels[ui]] = lbl\n",
    "    feed_dict[dec_label_masks[ui]] = (np.array([ui for _ in range(batch_size)])<du_lengths).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eu_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  4., 83.,  4.,  4.,  4., 83.,  4.,  4.,  4.,  4., 83.,  4.,\n",
       "        4.,  4.,  4.], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eu_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   55.,    59.,  6015.,    49.,  5135.,    17.,   356.,  1071.,\n",
       "          17., 14780.,    17.,  1583.,    17.,    31.,   136.,  1218.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_over_time = []\n",
    "num_epochs = 1000\n",
    "enc_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=max_src_length,is_source=True)\n",
    "dec_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=max_tgt_length,is_source=False)\n",
    "avg_loss = 0\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    sent_ids = np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
    "    # ====================== ENCODER DATA COLLECTION ================================================\n",
    "\n",
    "    eu_data, eu_labels, _, eu_lengths = enc_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "\n",
    "\n",
    "    feed_dict = {}\n",
    "    feed_dict[enc_train_inp_lengths] = eu_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(eu_data,eu_labels)):     \n",
    "        #print(ui)\n",
    "        feed_dict[enc_train_inputs[ui]] = dat                \n",
    "\n",
    "    # ====================== DECODER DATA COLLECTION ===========================\n",
    "\n",
    "    du_data, du_labels, _, du_lengths = dec_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "\n",
    "\n",
    "    feed_dict[dec_train_inp_lengths] = du_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(du_data,du_labels)):            \n",
    "        feed_dict[dec_train_inputs[ui]] = dat\n",
    "        feed_dict[dec_train_labels[ui]] = lbl\n",
    "        feed_dict[dec_label_masks[ui]] = (np.array([ui for _ in range(batch_size)])<du_lengths).astype(np.int32)\n",
    "        \n",
    "        \n",
    "    _,l,tr_pred = sess.run([adam_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "    tr_pred = tr_pred.flatten()\n",
    "    \n",
    "    \n",
    "    if (num_epochs+1)%250==0:\n",
    "        rand_idx = np.random.randint(low=1,high=batch_size)\n",
    "        print_str = 'Actual: '\n",
    "        for w in np.concatenate(du_labels,axis=0)[rand_idx::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "\n",
    "            \n",
    "        print()\n",
    "        print_str = 'Predicted: '\n",
    "        for w in tr_pred[rand_idx::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "        print()        \n",
    "        \n",
    "    avg_loss += l\n",
    "    \n",
    "    if (num_epochs+1)%500==0:\n",
    "        print('============= Step ', str(num_epochs+1), ' =============')\n",
    "        print('\\t Loss: ',avg_loss/500.0)\n",
    "        \n",
    "        loss_over_time.append(avg_loss/500.0)\n",
    "             \n",
    "        avg_loss = 0.0\n",
    "        sess.run(inc_gstep)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "name": "NMT.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
