{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "YPEz3w1I_5uE",
    "outputId": "e22d99e1-15f2-4309-c356-fd90a83628df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-21 18:51:36--  https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 717610118 (684M) [text/plain]\n",
      "Saving to: ‘train.de’\n",
      "\n",
      "train.de            100%[===================>] 684.37M  10.8MB/s    in 54s     \n",
      "\n",
      "2018-10-21 18:52:31 (12.6 MB/s) - ‘train.de’ saved [717610118/717610118]\n",
      "\n",
      "--2018-10-21 18:52:32--  https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 644937323 (615M) [text/plain]\n",
      "Saving to: ‘train.en’\n",
      "\n",
      "train.en            100%[===================>] 615.06M  13.4MB/s    in 74s     \n",
      "\n",
      "2018-10-21 18:53:46 (8.31 MB/s) - ‘train.en’ saved [644937323/644937323]\n",
      "\n",
      "--2018-10-21 18:53:46--  https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.en\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 403998 (395K) [text/plain]\n",
      "Saving to: ‘vocab.50K.en’\n",
      "\n",
      "vocab.50K.en        100%[===================>] 394.53K   877KB/s    in 0.5s    \n",
      "\n",
      "2018-10-21 18:53:47 (877 KB/s) - ‘vocab.50K.en’ saved [403998/403998]\n",
      "\n",
      "--2018-10-21 18:53:47--  https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.de\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 505304 (493K) [text/plain]\n",
      "Saving to: ‘vocab.50K.de’\n",
      "\n",
      "vocab.50K.de        100%[===================>] 493.46K  1.07MB/s    in 0.5s    \n",
      "\n",
      "2018-10-21 18:53:48 (1.07 MB/s) - ‘vocab.50K.de’ saved [505304/505304]\n",
      "\n",
      "total 1331536\n",
      "-rw-r--r-- 1 root root 644937323 Aug 22  2014 train.en\n",
      "-rw-r--r-- 1 root root 717610118 Aug 22  2014 train.de\n",
      "-rw-r--r-- 1 root root    403998 Jan 27  2015 vocab.50K.en\n",
      "-rw-r--r-- 1 root root    505304 Jan 27  2015 vocab.50K.de\n",
      "drwxr-xr-x 2 root root      4096 Oct 18 16:40 sample_data\n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de\n",
    "\n",
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en\n",
    "\n",
    "\n",
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.en\n",
    "\n",
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.de\n",
    "\n",
    "\n",
    "!ls -lrt\n",
    "\n",
    "#### All the necessary files have been downloaded.\n",
    "#### Now we will begin building the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qfHSsXwKfdX7"
   },
   "outputs": [],
   "source": [
    "# Hide all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "26zowsLBARNx"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Seq2Seq Items\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "16nutZFeCBII"
   },
   "outputs": [],
   "source": [
    "# Building the dictionary required for word lookup.\n",
    "# To dictionaries built - one for word to key and other for key to word. \n",
    "# The dictionary is built for both source and target language\n",
    "\n",
    "src_dictionary = dict()\n",
    "with open(\"vocab.50K.de\", encoding = \"utf-8\") as f:\n",
    "  word_count = 0\n",
    "  for line in f:\n",
    "    src_dictionary[line.split(\"\\n\")[0]] = word_count \n",
    "    word_count +=1\n",
    "\n",
    "src_reverse_dictionary = dict(zip(src_dictionary.values(), src_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "Mf8Rg8EDNiYP",
    "outputId": "d725c86a-2493-48fb-b31c-c36815ff4ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "das\n",
      "16\n",
      "Vocabulary:50000\n"
     ]
    }
   ],
   "source": [
    "# Check whether the above code worked right. \n",
    "# The word 'das' is mapped to 16\n",
    "\n",
    "print(src_reverse_dictionary[16])\n",
    "print(src_dictionary['das'])\n",
    "print(\"Vocabulary:\"+ str(len(src_dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gphF9bdJOMUu"
   },
   "outputs": [],
   "source": [
    "tgt_dictionary = dict()\n",
    "with open(\"vocab.50K.en\", encoding = \"utf-8\") as f:\n",
    "  word_count = 0\n",
    "  for line in f:\n",
    "    tgt_dictionary[line.split(\"\\n\")[0]] = word_count \n",
    "    word_count +=1\n",
    "\n",
    "tgt_reverse_dictionary = dict(zip(tgt_dictionary.values(), tgt_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "aPLAvQu9PHS7",
    "outputId": "a614a439-8bb7-4cf1-e302-d2f4ba029093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you\n",
      "28\n",
      "Vocabulary:50000\n"
     ]
    }
   ],
   "source": [
    "# Check whether the above code worked right. \n",
    "# The word 'you' is mapped to 28\n",
    "\n",
    "print(tgt_reverse_dictionary[28])\n",
    "print(tgt_dictionary['you'])\n",
    "print(\"Vocabulary:\"+ str(len(tgt_dictionary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9uZkxI-6RfvM"
   },
   "source": [
    "#### Now, lets load the german (source) and the english (target) sentences separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "SrcMGAAzRh4m",
    "outputId": "be86b7df-3ab8-4a79-93dc-3163bc2e40fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "source_sent = []\n",
    "no_sentences_to_be_read = 50000\n",
    "\n",
    "with open(\"train.de\", encoding = \"utf-8\") as f:\n",
    "    #count = 1\n",
    "    for line in f:\n",
    "        if len(source_sent) >= no_sentences_to_be_read:\n",
    "            break\n",
    "        line = line.split(\"\\n\")[0]\n",
    "        source_sent.append(line)\n",
    "        #count +=1\n",
    "\n",
    "print(len(source_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kpxRLJbWTQXK"
   },
   "outputs": [],
   "source": [
    "# The first 50 sentences were English for some reason\n",
    "source_sent = source_sent[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "rw9oipSod-bz",
    "outputId": "91e16f31-b755-48cb-80a3-384d3077dca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "target_sent = []\n",
    "\n",
    "with open(\"train.en\", encoding = \"utf-8\") as f:\n",
    "    #count = 1\n",
    "    for line in f:\n",
    "        if len(target_sent) >= no_sentences_to_be_read:\n",
    "            break\n",
    "        line = line.split(\"\\n\")[0]\n",
    "        target_sent.append(line)\n",
    "        #count +=1\n",
    "\n",
    "print(len(target_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2E6kXvE5etUy"
   },
   "outputs": [],
   "source": [
    "#The number of sentences must be same for both source and target\n",
    "target_sent = target_sent[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ujacwGDuev1_"
   },
   "outputs": [],
   "source": [
    "assert(len(source_sent) == len(target_sent))\n",
    "#len(target_sent)\n",
    "#len(source_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "e3N42HVXfxbc",
    "outputId": "e7788653-7598-4cb5-cbdd-420a4ad580a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (German): Heute verstehen sich QuarkXPress ® 8 , Photoshop ® und Illustrator ® besser als jemals zuvor . Dank HTML und CSS ­ können Anwender von QuarkXPress inzwischen alle Medien bedienen , und das unabhängig von Anwendungen der Adobe ® Creative Suite ® wie Adobe Flash ® ( SWF ) und Adobe Dreamweaver ® .\n",
      "Target (English): Today , QuarkXPress ® 8 has tighter integration with Photoshop ® and Illustrator ® than ever before , and through standards like HTML and CSS , QuarkXPress users can publish across media both independently and alongside Adobe ® Creative Suite ® applications like Adobe Flash ® ( SWF ) and Adobe Dreamweaver ® .\n",
      "\n",
      "\n",
      "Source (German): Es existieren Busverbindungen in nahezu jeden Ort der Provence ( eventuell mit Umsteigen in Aix ##AT##-##AT## en ##AT##-##AT## Provence ) , allerdings sollte beachtet werden , dass die letzten Busse abends ca. um 19 Uhr fahren .\n",
      "Target (English): As always in France those highways are expensive but practical , comfortable and fast .\n",
      "\n",
      "\n",
      "Source (German): Es war staubig , das Bad schmutzig . Sogar die Beleuchtung an der Wand im Flur ( Seitengebäude ) war richtig verstaubt .\n",
      "Target (English): It was rather old fashioned in the decoration .\n",
      "\n",
      "\n",
      "Source (German): Auch ist , so denkt Dr. Gutherz , bereits die erste Seite sehr viel versprechend , da sie eine Definition des klinischen Psychotrauma ##AT##-##AT## Begriffes enthält , der er gänzlich zustimmen kann .\n",
      "Target (English): At the rhetorical climax of this summary , Dr Goodheart comes across some sentences expressed with great pathos .\n",
      "\n",
      "\n",
      "Source (German): Bei einer digitalen Bildkette wird das Intensitätssignal für jedes Pixel ohne analoge Zwischenschritte direkt in der Detektoreinheit digitalisiert , d.h. in Zahlen umgewandelt .\n",
      "Target (English): A digital image chain is an image chain that is equipped with a digital detector instead of an analogue one .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets print some samples of both the source and target languages\n",
    "for i in range(0, len(source_sent), 10000):\n",
    "    print(\"Source (German):\", source_sent[i])\n",
    "    print(\"Target (English):\", target_sent[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2tpSKcXXgg67"
   },
   "outputs": [],
   "source": [
    "# Split the sentence into space separated tokens. \n",
    "# If tokens are not present in the vocabulary, then we add the <unk> unknown\n",
    "# token.\n",
    "def split_to_token(sentence, is_source):\n",
    "    sentence = sentence.replace(\",\",\" ,\")\n",
    "    sentence = sentence.replace(\".\",\" .\")\n",
    "    sentence = sentence.replace(\"\\n\",\" \")\n",
    "  \n",
    "    tokens = sentence.split(\" \")\n",
    "  \n",
    "    for i in range(len(tokens)):\n",
    "        if is_source:\n",
    "            if tokens[i] not in src_dictionary:\n",
    "                tokens[i] = '<unk>'\n",
    "    \n",
    "        else:\n",
    "            if tokens[i] not in tgt_dictionary:\n",
    "                tokens[i] = '<unk>'\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "colab_type": "code",
    "id": "up9CpTwMzfWv",
    "outputId": "d77f51ba-aab3-45c0-ad24-dd26bacbf899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length in source (German): 25.35941941941942\n",
      "Mean length in target: (English) 27.59089089089089\n",
      "\n",
      "\n",
      "Max length in source (German): 118\n",
      "Max length in target: (English) 119\n"
     ]
    }
   ],
   "source": [
    "# Finding the mean, max length of the full data\n",
    "source_len = []\n",
    "target_len = []\n",
    "\n",
    "# Number of source and target samples are same. \n",
    "for i in range(len(source_sent)):\n",
    "    source_len.append(len(split_to_token(source_sent[i], True)))\n",
    "    target_len.append(len(split_to_token(target_sent[i], False)))\n",
    "\n",
    "print(\"Mean length in source (German):\" , np.mean(source_len))\n",
    "print(\"Mean length in target: (English)\" , np.mean(target_len))\n",
    "print(\"\\n\")\n",
    "print(\"Max length in source (German):\" , np.max(source_len))\n",
    "print(\"Max length in target: (English)\" , np.max(target_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "I5fNShv4TUAZ",
    "outputId": "d6347e7e-6b89-41d9-a2b2-a3c667f48f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of source sentences: 49950\n",
      "Total number of target sentences: 49950\n",
      "Length of each source sentence: 40\n",
      "Length of each target sentence: 60\n"
     ]
    }
   ],
   "source": [
    "train_inputs = []\n",
    "train_outputs = []\n",
    "train_inp_lengths = []\n",
    "train_out_lengths = []\n",
    "\n",
    "max_src_length = 40\n",
    "max_tgt_length = 60\n",
    "\n",
    "for i in range(len(source_sent)):\n",
    "    src_tokens = split_to_token(source_sent[i], True)\n",
    "    target_tokens = split_to_token(target_sent[i], False)\n",
    "  \n",
    "    src_sentence_numbers = []\n",
    "    for token in src_tokens:\n",
    "        src_sentence_numbers.append(src_dictionary[token])\n",
    "    \n",
    "    \n",
    "    target_sentence_numbers = []\n",
    "    # Add a token which indicates the end of source and begining of the target.\n",
    "    target_sentence_numbers.append(tgt_dictionary['</s>'])\n",
    "  \n",
    "    for token in target_tokens:\n",
    "        target_sentence_numbers.append(tgt_dictionary[token])\n",
    "   \n",
    "    # Reverse the source sentence list for better performance/translation. \n",
    "    # This fact is based out of a paper for NMT.\n",
    "  \n",
    "    src_sentence_numbers = src_sentence_numbers[::-1]\n",
    "\n",
    "    # Add the start symbol at the start of source.\n",
    "    src_sentence_numbers.insert(0, src_dictionary['<s>'])\n",
    "    train_inp_lengths.append(min(len(src_sentence_numbers)+1,max_src_length))\n",
    "    # Make sure that both the source and target have same length.\n",
    "\n",
    "\n",
    "    if len(src_sentence_numbers) < max_src_length:\n",
    "        src_sentence_numbers.extend([ src_dictionary['</s>'] for i in range( max_src_length - len(src_sentence_numbers) )])\n",
    "\n",
    "    elif len(src_sentence_numbers) > max_src_length:\n",
    "        src_sentence_numbers = src_sentence_numbers[:max_src_length]\n",
    "\n",
    "\n",
    "    if len(target_sentence_numbers) < max_tgt_length:\n",
    "        target_sentence_numbers.extend([ tgt_dictionary['</s>'] for i in range( max_tgt_length - len(target_sentence_numbers) )])\n",
    "\n",
    "    elif len(target_sentence_numbers) > max_tgt_length:\n",
    "        target_sentence_numbers = target_sentence_numbers[:max_tgt_length]\n",
    "\n",
    "\n",
    "    if len(src_sentence_numbers) == max_src_length and len(target_sentence_numbers) == max_tgt_length:\n",
    "        train_inputs.append(src_sentence_numbers)\n",
    "        train_outputs.append(target_sentence_numbers)\n",
    "\n",
    "train_inp_lengths = np.array(train_inp_lengths, dtype=np.int32)\n",
    "print(\"Total number of source sentences:\", len(train_inputs))\n",
    "print(\"Total number of target sentences:\", len(train_outputs))\n",
    "print(\"Length of each source sentence:\", len(train_inputs[0]))\n",
    "print(\"Length of each target sentence:\", len(train_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DhPwJY2pAX9C"
   },
   "outputs": [],
   "source": [
    "# Convert the source and target to numpy values\n",
    "train_inputs = np.array(train_inputs, dtype=np.int32)\n",
    "train_outputs = np.array(train_outputs, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "iUGTJNGyB2yE",
    "outputId": "e3c19660-533c-48f4-89d6-ddf339178362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '.', '<unk>', '®', '<unk>', 'Adobe', 'und', ')', 'SWF', '(', '®', 'Flash', 'Adobe', 'wie', '®', 'Suite', 'Creative', '®', 'Adobe', 'der', 'Anwendungen', 'von', 'unabhängig', 'das', 'und', ',', '<unk>', 'bedienen', 'Medien', 'alle', 'inzwischen', 'QuarkXPress', 'von', 'Anwender', 'können', '\\xad', 'CSS', 'und', 'HTML', 'Dank']\n",
      "['</s>', 'Today', '<unk>', ',', 'QuarkXPress', '®', '8', 'has', 'tighter', 'integration', 'with', 'Photoshop', '®', 'and', 'Illustrator', '®', 'than', 'ever', 'before', '<unk>', ',', 'and', 'through', 'standards', 'like', 'HTML', 'and', 'CSS', '<unk>', ',', 'QuarkXPress', 'users', 'can', 'publish', 'across', 'media', 'both', 'independently', 'and', 'alongside', 'Adobe', '®', 'Creative', 'Suite', '®', 'applications', 'like', 'Adobe', 'Flash', '®', '(', 'SWF', ')', 'and', 'Adobe', 'Dreamweaver', '®', '<unk>', '.', '</s>']\n",
      "['<s>', '.', '<unk>', 'Mittelmeer', 'am', 'Katalonien', 'von', 'Zentrum', 'im', 'mitten', 'Halbinsel', 'iberischen', 'der', 'Nordosten', 'im', 'liegt', 'Barcelona', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', 'Barcelona', '<unk>', ',', 'Spain', 'is', 'a', 'city', 'located', 'at', 'the', 'northeast', 'side', 'of', 'the', 'Iberian', 'Peninsula', '<unk>', ',', 'in', 'the', 'heart', 'of', 'Catalonia', 'and', 'bordered', 'by', 'the', 'Mediterranean', 'Sea', 'to', 'the', 'east', '<unk>', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# See some sample source and target sentences.\n",
    "# The source sentence is reversed\n",
    "print([ src_reverse_dictionary[i] for i in train_inputs[0] ])\n",
    "print([ tgt_reverse_dictionary[i] for i in train_outputs[0] ])\n",
    "\n",
    "print([ src_reverse_dictionary[i] for i in train_inputs[100] ])\n",
    "print([ tgt_reverse_dictionary[i] for i in train_outputs[100] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "d5iRtVr4CWHK"
   },
   "outputs": [],
   "source": [
    "class DataGeneratorMT(object):\n",
    "\n",
    "    def __init__(self,batch_size,num_unroll,is_source):\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "\n",
    "        self._sent_ids = None\n",
    "\n",
    "        self._is_source = is_source\n",
    "\n",
    "\n",
    "    def print(self):\n",
    "        print(self._cursor)\n",
    "\n",
    "\n",
    "    def next_batch(self, sent_ids):\n",
    "        if self._is_source:\n",
    "            max_sent_length = max_src_length\n",
    "        else:\n",
    "            max_sent_length = max_tgt_length\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            sent_id = sent_ids[b]\n",
    "\n",
    "            if self._is_source:\n",
    "                sent_text = train_inputs[sent_id]\n",
    "                batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b] = sent_text[self._cursor[b] + 1]\n",
    "                \n",
    "            else:\n",
    "                sent_text = train_outputs[sent_id]\n",
    "                batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b] = sent_text[self._cursor[b] + 1]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b] + 1)%(max_sent_length - 1)\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "\n",
    "    def unroll_batches(self, sent_ids):\n",
    "        if sent_ids is not None:\n",
    "            self._sent_ids = sent_ids\n",
    "\n",
    "        unroll_data, unroll_labels = [], []\n",
    "        inp_lengths = None\n",
    "        for i in range(self._num_unroll):\n",
    "            data, labels = self.next_batch(self._sent_ids)\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "            inp_lengths = train_inp_lengths[sent_ids]\n",
    "\n",
    "        return unroll_data, unroll_labels, self._sent_ids, inp_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "5ndWtZ4FONzj",
    "outputId": "a637eb95-5d5e-4f77-c2ea-7881116f6060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data - German\n",
      "['<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['.', '.', '.', '.', '.']\n",
      "\n",
      "\n",
      "['.', '.', '.', '.', '.']\n",
      "['<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "\n",
      "\n",
      "['<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "['®', 'können', 'lässt', 'bietet', 'nutzen']\n",
      "\n",
      "\n",
      "['®', 'können', 'lässt', 'bietet', 'nutzen']\n",
      "['<unk>', 'nutzen', 'erschließen', 'Dateiformat', 'optimal']\n",
      "\n",
      "\n",
      "['<unk>', 'nutzen', 'erschließen', 'Dateiformat', 'optimal']\n",
      "['Adobe', 'QuarkXPress', 'Software', '##AT##-##AT##', 'Bilder']\n",
      "\n",
      "\n",
      "['Adobe', 'QuarkXPress', 'Software', '##AT##-##AT##', 'Bilder']\n",
      "['und', 'mit', '##AT##-##AT##', 'PSD', 'Ihre']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ob = DataGeneratorMT(5, 40, True)\n",
    "data, label, _, _ = ob.unroll_batches([0,1,2,3,4])\n",
    "#print(data)\n",
    "#print(label)\n",
    "\n",
    "count = 0\n",
    "print('Source data - German')\n",
    "for dta, lbl in zip(data,label):\n",
    "    if count > 5:\n",
    "        break\n",
    "    print([src_reverse_dictionary[w] for w in dta.tolist()])\n",
    "    print([src_reverse_dictionary[w] for w in lbl.tolist()])\n",
    "    print(\"\\n\")\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "tylYzPgffdY_"
   },
   "source": [
    "#### The above output looks fine. \n",
    "#### In seq2seq model, if we have [1,2,3,4,5] as input, using batch size of 3, we can have [1,2,3] , [2,3,4], [3,4,5] as batches.\n",
    "#### Because of this, we have the code:\n",
    "##### batch_data[b] = sent_text[self._cursor[b]]\n",
    "##### batch_labels[b] = sent_text[self._cursor[b] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "X1OMpNIefdZA",
    "outputId": "246a82d0-a7f9-4e0e-e433-bab76e7b6514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target data - English\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['Today', 'Here', 'You', 'QuarkXPress', 'In']\n",
      "\n",
      "\n",
      "['Today', 'Here', 'You', 'QuarkXPress', 'In']\n",
      "['<unk>', '<unk>', '’', '8', 'this']\n",
      "\n",
      "\n",
      "['<unk>', '<unk>', '’', '8', 'this']\n",
      "[',', ',', 'll', 'is', 'section']\n",
      "\n",
      "\n",
      "[',', ',', 'll', 'is', 'section']\n",
      "['QuarkXPress', 'you', 'be', 'considered', 'we']\n",
      "\n",
      "\n",
      "['QuarkXPress', 'you', 'be', 'considered', 'we']\n",
      "['®', '’', 'surprised', 'by', '’']\n",
      "\n",
      "\n",
      "['®', '’', 'surprised', 'by', '’']\n",
      "['8', 'll', 'how', 'many', 'll']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ob = DataGeneratorMT(5, 40, False)\n",
    "data, label, _, _ = ob.unroll_batches([0,1,2,3,4])\n",
    "#print(data)\n",
    "#print(label)\n",
    "\n",
    "count = 0\n",
    "print('Target data - English')\n",
    "for dta, lbl in zip(data,label):\n",
    "    if count > 5:\n",
    "        break\n",
    "    print([tgt_reverse_dictionary[w] for w in dta.tolist()])\n",
    "    print([tgt_reverse_dictionary[w] for w in lbl.tolist()])\n",
    "    print(\"\\n\")\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "elIry4zPhhar",
    "outputId": "2b7a9069-96a4-4232-efd2-c6c72bc868ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-21 18:54:17--  https://www.dropbox.com/s/01e7dndrxopguk6/en-embeddings.npy?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:6019:1::a27d:401\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/01e7dndrxopguk6/en-embeddings.npy [following]\n",
      "--2018-10-21 18:54:17--  https://www.dropbox.com/s/raw/01e7dndrxopguk6/en-embeddings.npy\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uce88980a4a1f9f35b2cca7c9195.dl.dropboxusercontent.com/cd/0/inline/ATlqgCMmyAUwMPO7HSSI_EARQ3JiL1CbAIjZEEvs-5A-SzY2HgBlT_EbwvZPx12PIXgSKxo23uYtRzG6-HdLg75eX3IGeDk3hufLOvoo6tSniYPJQPBmdy-CQHBgrRPfk_P9x-GJSo-OWpM15S-iSLdjF29txgqp2-i_5nQjHaAe0mc-qg43YEiQcdwSeJQ-yhE/file [following]\n",
      "--2018-10-21 18:54:18--  https://uce88980a4a1f9f35b2cca7c9195.dl.dropboxusercontent.com/cd/0/inline/ATlqgCMmyAUwMPO7HSSI_EARQ3JiL1CbAIjZEEvs-5A-SzY2HgBlT_EbwvZPx12PIXgSKxo23uYtRzG6-HdLg75eX3IGeDk3hufLOvoo6tSniYPJQPBmdy-CQHBgrRPfk_P9x-GJSo-OWpM15S-iSLdjF29txgqp2-i_5nQjHaAe0mc-qg43YEiQcdwSeJQ-yhE/file\n",
      "Resolving uce88980a4a1f9f35b2cca7c9195.dl.dropboxusercontent.com (uce88980a4a1f9f35b2cca7c9195.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6018:6::a27d:306\n",
      "Connecting to uce88980a4a1f9f35b2cca7c9195.dl.dropboxusercontent.com (uce88980a4a1f9f35b2cca7c9195.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: /cd/0/inline2/ATmY8KTiLTOfEB5J4OdmVqR8caYN8X_iYikAsLKkl2iU74kFAJ8m1CflIbCsqn6tlmrMtd0gt3hF8zB2U5Z-31vS-CiKkTec_43XwxMN4a1U2PNQwN8ZLjANQ0P8j-__b7A38oatCROaQIaqNk7LikYgaqvVG7E3DA4xjPbfeaztCLBpg11gbgU4g6WHQMuQDsi_QNE0iRmE9IPB5r2vl5z0j7IFyrvpfcsQXNeObHprBU5rauOFYbDXLpvNXyCF_g5eShwm4HI-0pp__X4FTrmGKrC-UJZw_MapdJWi7_nlzI-pFvYrVO8TlE0CNqWAz4dSq5-dYqYqhyf41PBlqQ_6Vc3jaCT4LAt1Ck-8c7rY_h-sd1eD3DyKPux3m4fP6e3ivkpf31LTQQPx1KytFYzf8XkqzxEiI9JE88CN_5NAGm8J8T6QTfeyvepR7HHExck/file [following]\n",
      "--2018-10-21 18:54:18--  https://uce88980a4a1f9f35b2cca7c9195.dl.dropboxusercontent.com/cd/0/inline2/ATmY8KTiLTOfEB5J4OdmVqR8caYN8X_iYikAsLKkl2iU74kFAJ8m1CflIbCsqn6tlmrMtd0gt3hF8zB2U5Z-31vS-CiKkTec_43XwxMN4a1U2PNQwN8ZLjANQ0P8j-__b7A38oatCROaQIaqNk7LikYgaqvVG7E3DA4xjPbfeaztCLBpg11gbgU4g6WHQMuQDsi_QNE0iRmE9IPB5r2vl5z0j7IFyrvpfcsQXNeObHprBU5rauOFYbDXLpvNXyCF_g5eShwm4HI-0pp__X4FTrmGKrC-UJZw_MapdJWi7_nlzI-pFvYrVO8TlE0CNqWAz4dSq5-dYqYqhyf41PBlqQ_6Vc3jaCT4LAt1Ck-8c7rY_h-sd1eD3DyKPux3m4fP6e3ivkpf31LTQQPx1KytFYzf8XkqzxEiI9JE88CN_5NAGm8J8T6QTfeyvepR7HHExck/file\n",
      "Reusing existing connection to uce88980a4a1f9f35b2cca7c9195.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 25600080 (24M) [application/octet-stream]\n",
      "Saving to: ‘en-embeddings.npy’\n",
      "\n",
      "en-embeddings.npy   100%[===================>]  24.41M  52.8MB/s    in 0.5s    \n",
      "\n",
      "2018-10-21 18:54:20 (52.8 MB/s) - ‘en-embeddings.npy’ saved [25600080/25600080]\n",
      "\n",
      "--2018-10-21 18:54:20--  https://www.dropbox.com/s/4204d2kgfknu4ci/de-embeddings.npy?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:6019:1::a27d:401\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/4204d2kgfknu4ci/de-embeddings.npy [following]\n",
      "--2018-10-21 18:54:20--  https://www.dropbox.com/s/raw/4204d2kgfknu4ci/de-embeddings.npy\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucaf04e160cd5850096cc3af264a.dl.dropboxusercontent.com/cd/0/inline/ATmUHedBwzSoqLvi-Nrzw3MTYqRI9PIREegWCC6A88MoWsOHxCbMTMgVXWRN6MPcDSnw56_JWVIvan_ds3slxRQ0NcLWHoOadQffgEvNBP_0a1Mf-2jlz-2f2A_UwSBPrCyPX6bsCzBCDbtcguj_gFq2SvcAO0nz16VmmnD0hx-9oVj_81wOgPz9tVR8nP6Z1pA/file [following]\n",
      "--2018-10-21 18:54:21--  https://ucaf04e160cd5850096cc3af264a.dl.dropboxusercontent.com/cd/0/inline/ATmUHedBwzSoqLvi-Nrzw3MTYqRI9PIREegWCC6A88MoWsOHxCbMTMgVXWRN6MPcDSnw56_JWVIvan_ds3slxRQ0NcLWHoOadQffgEvNBP_0a1Mf-2jlz-2f2A_UwSBPrCyPX6bsCzBCDbtcguj_gFq2SvcAO0nz16VmmnD0hx-9oVj_81wOgPz9tVR8nP6Z1pA/file\n",
      "Resolving ucaf04e160cd5850096cc3af264a.dl.dropboxusercontent.com (ucaf04e160cd5850096cc3af264a.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6018:6::a27d:306\n",
      "Connecting to ucaf04e160cd5850096cc3af264a.dl.dropboxusercontent.com (ucaf04e160cd5850096cc3af264a.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: /cd/0/inline2/ATmMUNjYIwLWSAGtDEPqDiajf4p8P9Aps3K6NCk4AvidVCImvOiUP1_gTrm_1UdSxad2GSXRjg4-GfxVpxaPoFnrfJyYO4nNwuZcF41txvezN9aNAaqI1fU0xnITINtRkzTKzsF7TT_oO6kQxgJcSuqHGK6JCbSx3Vmfh4IEzjwRPYrFYQJVg449W2_UXHRS0MOzMkIZcL8NEI_GS8KL1LeE7bIS8wz4gXluL8wBAuz6G2rjbNksNQTjPaSuXU0wgzp0mWywHQuFBHpJaegAUeTqsGiP23va6SZu-kN6cqXi8lMBxmRcMxcoSkXg3iyiQXG71Hh6V4mJ2gDG9aGLbMLx1klmMMB6sRLcsrciqzIgPkjE5lDHZbF6pkwxBOK3uv8qthIVjXQztjG1jeKwdrmOkesloRnd4Tyg4odQFEx4r4VBsQUpTo4Vx0XuGfHHp-Q/file [following]\n",
      "--2018-10-21 18:54:22--  https://ucaf04e160cd5850096cc3af264a.dl.dropboxusercontent.com/cd/0/inline2/ATmMUNjYIwLWSAGtDEPqDiajf4p8P9Aps3K6NCk4AvidVCImvOiUP1_gTrm_1UdSxad2GSXRjg4-GfxVpxaPoFnrfJyYO4nNwuZcF41txvezN9aNAaqI1fU0xnITINtRkzTKzsF7TT_oO6kQxgJcSuqHGK6JCbSx3Vmfh4IEzjwRPYrFYQJVg449W2_UXHRS0MOzMkIZcL8NEI_GS8KL1LeE7bIS8wz4gXluL8wBAuz6G2rjbNksNQTjPaSuXU0wgzp0mWywHQuFBHpJaegAUeTqsGiP23va6SZu-kN6cqXi8lMBxmRcMxcoSkXg3iyiQXG71Hh6V4mJ2gDG9aGLbMLx1klmMMB6sRLcsrciqzIgPkjE5lDHZbF6pkwxBOK3uv8qthIVjXQztjG1jeKwdrmOkesloRnd4Tyg4odQFEx4r4VBsQUpTo4Vx0XuGfHHp-Q/file\n",
      "Reusing existing connection to ucaf04e160cd5850096cc3af264a.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 25600080 (24M) [application/octet-stream]\n",
      "Saving to: ‘de-embeddings.npy’\n",
      "\n",
      "de-embeddings.npy   100%[===================>]  24.41M  55.8MB/s    in 0.4s    \n",
      "\n",
      "2018-10-21 18:54:23 (55.8 MB/s) - ‘de-embeddings.npy’ saved [25600080/25600080]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O en-embeddings.npy https://www.dropbox.com/s/01e7dndrxopguk6/en-embeddings.npy?dl=0\n",
    "!wget -O de-embeddings.npy https://www.dropbox.com/s/4204d2kgfknu4ci/de-embeddings.npy?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150
    },
    "colab_type": "code",
    "id": "UMy1_RoSkZvg",
    "outputId": "68ff0827-1fd3-490c-ff0c-fe7fcdf482d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1381552\n",
      "-rw-r--r-- 1 root root 644937323 Aug 22  2014 train.en\n",
      "-rw-r--r-- 1 root root 717610118 Aug 22  2014 train.de\n",
      "-rw-r--r-- 1 root root    403998 Jan 27  2015 vocab.50K.en\n",
      "-rw-r--r-- 1 root root    505304 Jan 27  2015 vocab.50K.de\n",
      "drwxr-xr-x 2 root root      4096 Oct 18 16:40 sample_data\n",
      "-rw-r--r-- 1 root root  25600080 Oct 21 18:54 en-embeddings.npy\n",
      "-rw-r--r-- 1 root root  25600080 Oct 21 18:54 de-embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "!ls -lrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "COWPvICrfdZE"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Load pre-trained word embeddings for german and english\n",
    "encoder_emb_layer = tf.convert_to_tensor(np.load('de-embeddings.npy'))\n",
    "decoder_emb_layer = tf.convert_to_tensor(np.load('en-embeddings.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "id": "PXw4dSqsfdZG",
    "outputId": "48c0f775-942d-4cb1-8555-33c38ebe872e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (German) Embeddings\n",
      "[ 0.02807283  0.08129065  0.04325406  0.03861211 -0.04374592  0.08648341\n",
      " -0.12594177  0.05204354  0.02891895 -0.00274239  0.0464763  -0.0177199\n",
      "  0.0113791   0.10005165 -0.13852786  0.07391532  0.14600192 -0.07613634\n",
      "  0.0165039   0.09500151 -0.09135051  0.06103227 -0.09518221 -0.00840024\n",
      "  0.1021672  -0.09210443  0.05864106  0.02367448 -0.12617454  0.03162083\n",
      " -0.00553827 -0.06233861 -0.09098011  0.04980104 -0.08403688 -0.02544336\n",
      " -0.03200009 -0.36211336  0.04187389  0.13499737  0.01335561  0.05164875\n",
      "  0.07950916  0.04037105  0.07873604  0.01508441 -0.01101452 -0.02970273\n",
      " -0.11738252  0.02539947 -0.10869873 -0.04156078  0.0270797   0.09760202\n",
      "  0.01272728  0.12135591 -0.09459837 -0.08765817  0.04319254 -0.04632879\n",
      "  0.03997531 -0.09053234 -0.05756423 -0.20108807 -0.04703222 -0.05928725\n",
      " -0.00646044  0.14901383  0.05235337 -0.00089508 -0.05842103 -0.01100476\n",
      "  0.11242474 -0.09033585 -0.04254638 -0.15847538 -0.12196966 -0.10012189\n",
      "  0.04274154 -0.10345502  0.00574398  0.07736626  0.11999135 -0.08652569\n",
      " -0.03872042  0.10062841  0.09217047  0.03666938 -0.04574378  0.06360335\n",
      " -0.08939499  0.10636167  0.01082712 -0.11277407  0.02705009 -0.02527077\n",
      " -0.24460979 -0.05275669  0.05250627 -0.04010779 -0.04981223 -0.15472683\n",
      "  0.15857296 -0.02322546  0.05232251 -0.04510524  0.15454935 -0.02650067\n",
      " -0.0042775   0.10024884 -0.03968636  0.06309864  0.09832346 -0.02791295\n",
      " -0.14186773 -0.02850333  0.05871908  0.08428557  0.04390426 -0.07982356\n",
      "  0.08855276 -0.04197617  0.07706168 -0.04147149  0.18912494  0.05615143\n",
      "  0.1390056   0.08622716]\n",
      "Number of Embeddings: 50000\n",
      "Source Embedding Size: 128\n",
      "Target Embedding Size: 128\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()  \n",
    "print(\"Source (German) Embeddings\")\n",
    "print(encoder_emb_layer.eval()[0])\n",
    "#print(\"Target (English) Embeddings\")\n",
    "#print(decoder_emb_layer.eval()[0])\n",
    "\n",
    "# Check if the length of source and target embeddings are same.\n",
    "assert(len(encoder_emb_layer.eval()) == len(decoder_emb_layer.eval()))\n",
    "assert(len(encoder_emb_layer.eval()[0]) == len(decoder_emb_layer.eval()[0]))\n",
    "\n",
    "print(\"Number of Embeddings:\", len(encoder_emb_layer.eval()))\n",
    "print(\"Source Embedding Size:\",len(encoder_emb_layer.eval()[0]))\n",
    "print(\"Target Embedding Size:\",len(decoder_emb_layer.eval()[0]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "2j7GJ2lofdZK",
    "outputId": "e4c69f39-f1d0-4209-b600-6e6663b7b098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of encoder train inputs: 40 x 16\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "enc_train_inputs = []\n",
    "dec_train_inputs = []\n",
    "dec_train_labels=[]\n",
    "dec_label_masks = []\n",
    "\n",
    "print(\"Dimension of encoder train inputs:\", max_src_length, \"x\",batch_size)\n",
    "for i in range(max_src_length):\n",
    "    enc_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size],name='enc_train_inputs_%d'%i))\n",
    "\n",
    "print(len(enc_train_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "sM_ZtQFOfdZQ",
    "outputId": "18929d16-69b2-4de6-f01f-2b543ab7c939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of decoder train inputs: 60 x 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of decoder train inputs:\", max_tgt_length, \"x\",batch_size)\n",
    "for i in range(max_tgt_length):\n",
    "    dec_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size], name='dec_train_inputs_%d'%i))\n",
    "    dec_train_labels.append(tf.placeholder(tf.int32, shape=[batch_size], name='dec_train_labels_%d'%i))\n",
    "    dec_label_masks.append(tf.placeholder(tf.float32, shape=[batch_size], name='dec_label_masks_%d'%i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "id": "dXRBXXaOfdZU",
    "outputId": "1991ce6f-4d99-42e6-fcb1-8907a61b7f91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02807283  0.08129065  0.04325406  0.03861211 -0.04374592  0.08648341\n",
      " -0.12594177  0.05204354  0.02891895 -0.00274239  0.0464763  -0.0177199\n",
      "  0.0113791   0.10005165 -0.13852786  0.07391532  0.14600192 -0.07613634\n",
      "  0.0165039   0.09500151 -0.09135051  0.06103227 -0.09518221 -0.00840024\n",
      "  0.1021672  -0.09210443  0.05864106  0.02367448 -0.12617454  0.03162083\n",
      " -0.00553827 -0.06233861 -0.09098011  0.04980104 -0.08403688 -0.02544336\n",
      " -0.03200009 -0.36211336  0.04187389  0.13499737  0.01335561  0.05164875\n",
      "  0.07950916  0.04037105  0.07873604  0.01508441 -0.01101452 -0.02970273\n",
      " -0.11738252  0.02539947 -0.10869873 -0.04156078  0.0270797   0.09760202\n",
      "  0.01272728  0.12135591 -0.09459837 -0.08765817  0.04319254 -0.04632879\n",
      "  0.03997531 -0.09053234 -0.05756423 -0.20108807 -0.04703222 -0.05928725\n",
      " -0.00646044  0.14901383  0.05235337 -0.00089508 -0.05842103 -0.01100476\n",
      "  0.11242474 -0.09033585 -0.04254638 -0.15847538 -0.12196966 -0.10012189\n",
      "  0.04274154 -0.10345502  0.00574398  0.07736626  0.11999135 -0.08652569\n",
      " -0.03872042  0.10062841  0.09217047  0.03666938 -0.04574378  0.06360335\n",
      " -0.08939499  0.10636167  0.01082712 -0.11277407  0.02705009 -0.02527077\n",
      " -0.24460979 -0.05275669  0.05250627 -0.04010779 -0.04981223 -0.15472683\n",
      "  0.15857296 -0.02322546  0.05232251 -0.04510524  0.15454935 -0.02650067\n",
      " -0.0042775   0.10024884 -0.03968636  0.06309864  0.09832346 -0.02791295\n",
      " -0.14186773 -0.02850333  0.05871908  0.08428557  0.04390426 -0.07982356\n",
      "  0.08855276 -0.04197617  0.07706168 -0.04147149  0.18912494  0.05615143\n",
      "  0.1390056   0.08622716]\n"
     ]
    }
   ],
   "source": [
    "# Sample code to read and display an embedding\n",
    "sample_embedding = tf.nn.embedding_lookup(encoder_emb_layer, 0)\n",
    "sess = tf.InteractiveSession()\n",
    "print(sample_embedding.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "FTVuy3YQfdZY",
    "outputId": "469ffbf8-04c8-4523-e863-61aaaf04f881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup_1/Identity:0\", shape=(16, 128), dtype=float32)\n",
      "Tensor(\"stack:0\", shape=(40, 16, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Each word_int is of length 16. 16 id's are passed to embedding_lookup to get their 128 dimension embedding.\n",
    "# So , the embedding_lookup returns 16x128 vector. \n",
    "# This process to repeated 40 times - which is the length of enc_train_inputs\n",
    "encoder_emb_inp = [tf.nn.embedding_lookup(encoder_emb_layer, word_int) for word_int in enc_train_inputs]\n",
    "print(encoder_emb_inp[0])\n",
    "encoder_emb_inp = tf.stack(encoder_emb_inp)\n",
    "# All the 40 elements are stacked to form a single tensor.\n",
    "print(encoder_emb_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "I5Ow1wK3fdZc",
    "outputId": "28a4a502-3531-435c-b236-7377dee6b4ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup_41/Identity:0\", shape=(16, 128), dtype=float32)\n",
      "Tensor(\"stack_1:0\", shape=(60, 16, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoder_emb_inp = [tf.nn.embedding_lookup(decoder_emb_layer, word_int) for word_int in dec_train_inputs]\n",
    "print(decoder_emb_inp[0])\n",
    "decoder_emb_inp = tf.stack(decoder_emb_inp)\n",
    "print(decoder_emb_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "WHWx3JuIfdZg",
    "outputId": "13837482-56a6-4e6c-adfa-aa8a527a78b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"train_input_lengths:0\", shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "enc_train_inp_lengths = tf.placeholder(tf.int32, shape=[batch_size],name='train_input_lengths')\n",
    "dec_train_inp_lengths = tf.placeholder(tf.int32, shape=[batch_size],name='train_output_lengths')\n",
    "print(enc_train_inp_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1AgRR8bfdZj"
   },
   "source": [
    "#### The input and output layers end here.\n",
    "#### Next we begin the encoder, decoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UXOZcENfdZj"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "2RCML-xwfdZk",
    "outputId": "f3583642-4c08-481e-8137-1ff1ef2076e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-33-b69050d74f52>:5: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "LSTMStateTuple(c=<tf.Tensor 'BasicLSTMCellZeroState/zeros:0' shape=(16, 128) dtype=float32>, h=<tf.Tensor 'BasicLSTMCellZeroState/zeros_1:0' shape=(16, 128) dtype=float32>)\n",
      "\n",
      "\n",
      "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(16, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(16, 128) dtype=float32>)\n",
      "\n",
      "\n",
      "\n",
      "Tensor(\"rnn/TensorArrayStack/TensorArrayGatherV3:0\", shape=(40, 16, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_units = 128 \n",
    "# num_units  - Parameter of BasicLSTMCell - Number of hidden neurons in each cell\n",
    "# Ideally, its size should be the size of the embeddings.\n",
    "\n",
    "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=num_units)\n",
    "initial_state = encoder_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "print(initial_state)\n",
    "\n",
    "# Reference - https://stackoverflow.com/questions/41885519/tensorflow-dynamic-rnn-parameters-meaning\n",
    "\"\"\"\n",
    "cell -            Each cell of the sequential modelling. In our case, each cell is a Basic LSTM Cell with \n",
    "                  128 neurons.\n",
    "\n",
    "inputs -          Its the input for the entire sequence model.\n",
    "                  If we use, time_major = True then dimension of encoder_emb_inp should be \n",
    "                  (max_time, batch_size, input_size - each embedding size) = (40, 16, 128)\n",
    "        \n",
    "initial_state  -  The hidden states of all the LSTM cells for each sequence. Hidden states are initialized\n",
    "                  for each sequence i.e., no hidden state is passed between the sequences even if they are\n",
    "                  from the same batch.\n",
    "                 \n",
    "sequence_length - is a vector of size batch_size in which each element gives the length of each sequence in \n",
    "                  the batch. \n",
    "\n",
    "swap_memory     - If True, Tensors are swapped between CPU and GPU.\n",
    "\"\"\"\n",
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "    cell= encoder_cell, inputs = encoder_emb_inp, initial_state=initial_state,\n",
    "    sequence_length=enc_train_inp_lengths, \n",
    "    time_major=True, swap_memory=True)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(encoder_state)\n",
    "print(\"\\n\\n\")\n",
    "print(encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsQhhvPjfdZn"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "eaIr-gA3fdZo",
    "outputId": "44d798c9-bdad-472c-d6c4-cbab174c3f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fcdbe66fb00>\n",
      "<tensorflow.python.layers.core.Dense object at 0x7fcdbe66fda0>\n"
     ]
    }
   ],
   "source": [
    "decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "print(decoder_cell)\n",
    "vocab_size = 50000\n",
    "\n",
    "# Projection layer is used to get the output of every LSTM cell\n",
    "projection_layer = Dense(units=vocab_size, use_bias=True)\n",
    "print(projection_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "wMfjBL03fdZr",
    "outputId": "da6662d2-3542-43c1-d85b-39a8522ddd3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60]\n"
     ]
    }
   ],
   "source": [
    "seq_len_vector_for_helper = [max_tgt_length for _ in range(batch_size)]\n",
    "print(seq_len_vector_for_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "IMlfUGaffdZu",
    "outputId": "fb5f7972-e2a7-4e0b-b99d-9ac8ea1e45d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(decoder_emb_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "3DMbmRMHfdZx",
    "outputId": "21067939-fa76-4115-c27b-0d0e8a574a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoder object at 0x7fcdbe66f898>\n"
     ]
    }
   ],
   "source": [
    "# TrainingHelper is used to feed the ground truth at every step instead of the decoded output \n",
    "# value from the previous step.\n",
    "# Reference- https://stackoverflow.com/questions/43826784/trouble-understanding-tf-contrib-seq2seq-traininghelper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_emb_inp, sequence_length= seq_len_vector_for_helper, \n",
    "                                           time_major= True)\n",
    "#Final encoder state becomes the first input for Decoder.\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection_layer)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4jruCsqDfdZz"
   },
   "outputs": [],
   "source": [
    "\n",
    "outputs, final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, output_time_major=True, swap_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "i3RBoJoGfdZ3"
   },
   "outputs": [],
   "source": [
    "logits = outputs.rnn_output\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=dec_train_labels, logits=logits)\n",
    "loss = tf.reduce_sum(cross_entropy * tf.stack(dec_label_masks))/(batch_size * max_tgt_length)\n",
    "train_prediction = outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "f14s4bY2fdZ7",
    "outputId": "70ce8931-5a62-4027-8adc-306400d1e236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Optimizer\n"
     ]
    }
   ],
   "source": [
    "print('Defining Optimizer')\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    0.01, global_step, decay_steps=10, decay_rate=0.9, staircase=True)\n",
    "\n",
    "with tf.variable_scope('Adam'):\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "#Reference - https://www.tensorflow.org/api_docs/python/tf/train/Optimizer    \n",
    "    \n",
    "# When we zip, all the gradients are grouped in one tuple\n",
    "# and all the variables are grouped in one.\n",
    "# adam_gradients = (v1_grad, v2_grad, v3_grad...) \n",
    "# variable = (v1, v2, v3...)\n",
    "# We do this to apply gradient clipping on all the gradients. \n",
    "adam_gradients, variable = zip(*adam_optimizer.compute_gradients(loss))\n",
    "adam_gradients, _ = tf.clip_by_global_norm(adam_gradients, 25.0)\n",
    "\n",
    "#We convert back to the original form of [(grad1, variable1), (grad2, v)....] to apply gradients\n",
    "adam_optimize = adam_optimizer.apply_gradients(zip(adam_gradients, variable))\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sADDsauwfdZ_"
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "GKDbaQBjfdaB",
    "outputId": "c4e257f0-6cfa-4968-bd0f-580cb15a5071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[14067 16881 45752 36252 37743  8335 11107 31424 43271 32861 29950 18223\n",
      "  1644 13117 23985 10072]\n",
      "[24 18 23 10 40 22 17 40 40 31 40 40 34 40 21 18]\n"
     ]
    }
   ],
   "source": [
    "enc_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=max_src_length,is_source=True)\n",
    "dec_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=max_tgt_length,is_source=False)\n",
    "\n",
    "enc_data_generator.print()\n",
    "\n",
    "sent_ids = np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
    "print(sent_ids)\n",
    "# ====================== ENCODER DATA COLLECTION ================================================\n",
    "\n",
    "eu_data, eu_labels, _, eu_lengths = enc_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "feed_dict[enc_train_inp_lengths] = eu_lengths\n",
    "for ui,(dat,lbl) in enumerate(zip(eu_data,eu_labels)):     \n",
    "    #print(ui)\n",
    "    feed_dict[enc_train_inputs[ui]] = dat                \n",
    "\n",
    "# ====================== DECODER DATA COLLECTION ===========================\n",
    "\n",
    "du_data, du_labels, _, du_lengths = dec_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "print(du_lengths)\n",
    "\n",
    "feed_dict[dec_train_inp_lengths] = du_lengths\n",
    "for ui,(dat,lbl) in enumerate(zip(du_data,du_labels)):            \n",
    "    feed_dict[dec_train_inputs[ui]] = dat\n",
    "    feed_dict[dec_train_labels[ui]] = lbl\n",
    "    feed_dict[dec_label_masks[ui]] = (np.array([ui for _ in range(batch_size)])<du_lengths).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "TrDwwx2kfdaE",
    "outputId": "e332da38-6494-4b6f-9b2c-49e6062985f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eu_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "qvHQfuaNfdaN",
    "outputId": "993f0597-17a3-4e9a-a9c5-8767198da4c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eu_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "5Xc0z7BgfdaS",
    "outputId": "cd8ee4ff-047b-40ee-872f-477f1029f860"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "mHc89DM2fdaW",
    "outputId": "c03446f4-2757-4ea2-941f-5b77fb5c82ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  318.,  1754.,    17.,  2567.,   136.,   760.,    17.,     0.,\n",
       "         177.,    17.,   241.,    49.,  1493.,    91.,    17., 14401.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2036
    },
    "colab_type": "code",
    "id": "Qg7E4TIlfdab",
    "outputId": "0cd2f7f2-f361-4de9-9330-f8b53b36895c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: unloading of fruit <unk> . A retractable balcony was also installed for safety purposes <unk> . </s> \n",
      "\n",
      "Predicted: <unk> <unk> the <unk> , </s> \n",
      "\n",
      "============= Step  500  =============\n",
      "\t Loss:  1.5996628906428814\n",
      "Actual: </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  1000  =============\n",
      "\t Loss:  1.3155119487941265\n",
      "Actual: </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  1500  =============\n",
      "\t Loss:  1.258278371937573\n",
      "Actual: </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  2000  =============\n",
      "\t Loss:  1.173017665296793\n",
      "Actual: </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  2500  =============\n",
      "\t Loss:  1.1739949850440026\n",
      "Actual: </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  3000  =============\n",
      "\t Loss:  1.112998608469963\n",
      "Actual: sites <unk> , ecommerce ASPs <unk> , purchase systems <unk> , etc ) downloading this ICEcat data ##AT##-##AT## sheet since 23 Mar 2007 <unk> . </s> \n",
      "\n",
      "Predicted: <unk> <unk> , the ASPs <unk> , purchase systems <unk> , etc ) downloading this ICEcat data ##AT##-##AT## sheet since 27 Sep 2005 <unk> . </s> \n",
      "\n",
      "============= Step  3500  =============\n",
      "\t Loss:  1.1558712753653526\n",
      "Actual: </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  4000  =============\n",
      "\t Loss:  1.0711237744390965\n",
      "Actual: &quot; ( Charming Spain Hotels ) <unk> . </s> \n",
      "\n",
      "Predicted: , <unk> <unk> ) ) ) <unk> . </s> \n",
      "\n",
      "============= Step  4500  =============\n",
      "\t Loss:  1.0960265347510576\n",
      "Actual: , Heating <unk> , Luggage Storage <unk> , Shops in Hotel <unk> , All Public and Restaurant <unk> , Bar <unk> , 24 ##AT##-##AT## Hour Front Desk <unk> , Garden <unk> , Terrace <unk> , Non ##AT##-##AT## Smoking Rooms <unk> , Family Rooms <unk> , Express Check ##AT##-##AT## In / Check ##AT##-##AT## Out <unk> , Safety Deposit Box <unk> , \n",
      "\n",
      "Predicted: , Ironing <unk> , Luggage Storage <unk> , All in the <unk> <unk> Ironing Public and </s> \n",
      "\n",
      "============= Step  5000  =============\n",
      "\t Loss:  1.0528401115238666\n",
      "Actual: pattern <unk> . </s> \n",
      "\n",
      "Predicted: to <unk> , </s> \n",
      "\n",
      "============= Step  5500  =============\n",
      "\t Loss:  1.0617465589046478\n",
      "Actual: </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  6000  =============\n",
      "\t Loss:  1.0224682049006224\n",
      "Actual: waived if you get permission from the copyright holder <unk> . </s> \n",
      "\n",
      "Predicted: used to you can to with the <unk> of <unk> . </s> \n",
      "\n",
      "============= Step  6500  =============\n",
      "\t Loss:  1.0527456856667996\n",
      "Actual: </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  7000  =============\n",
      "\t Loss:  1.0280824519246816\n",
      "Actual: architecture with small size <unk> , extreme <unk> <unk> , and hot ##AT##-##AT## <unk> industrial I / O and communication modules <unk> . </s> \n",
      "\n",
      "Predicted: for <unk> the NI <unk> . and with , , and <unk> ##AT##-##AT## screen <unk> <unk> <unk> O <unk> <unk> <unk> <unk> . </s> \n",
      "\n",
      "============= Step  7500  =============\n",
      "\t Loss:  1.0480252785384656\n",
      "Actual: </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  8000  =============\n",
      "\t Loss:  0.9938144543543458\n",
      "Actual: want to see boss fights <unk> , or maybe they have dial ##AT##-##AT## up and don &apos;t want to wait a week before they can watch anything <unk> . </s> \n",
      "\n",
      "Predicted: the to be the <unk> <unk> . and the the are been <unk> free <unk> <unk> &apos;t want to be to lot <unk> the are &apos;t the <unk> . </s> \n",
      "\n",
      "============= Step  8500  =============\n",
      "\t Loss:  1.012601620823145\n",
      "Actual: </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  9000  =============\n",
      "\t Loss:  0.9790593392699957\n",
      "Actual: Easy to reach from the airport with S1 and U ##AT##-##AT## Bahn <unk> . </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  9500  =============\n",
      "\t Loss:  0.986233556330204\n",
      "Actual: </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  10000  =============\n",
      "\t Loss:  0.9973243969529867\n"
     ]
    }
   ],
   "source": [
    "loss_over_time = []\n",
    "num_epochs = 10000\n",
    "enc_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=max_src_length,is_source=True)\n",
    "dec_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=max_tgt_length,is_source=False)\n",
    "avg_loss = 0\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    sent_ids = np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
    "    # ====================== ENCODER DATA COLLECTION ================================================\n",
    "\n",
    "    eu_data, eu_labels, _, eu_lengths = enc_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "\n",
    "\n",
    "    feed_dict = {}\n",
    "    feed_dict[enc_train_inp_lengths] = eu_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(eu_data,eu_labels)):     \n",
    "        #print(ui)\n",
    "        feed_dict[enc_train_inputs[ui]] = dat                \n",
    "\n",
    "    # ====================== DECODER DATA COLLECTION ===========================\n",
    "\n",
    "    du_data, du_labels, _, du_lengths = dec_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "\n",
    "\n",
    "    feed_dict[dec_train_inp_lengths] = du_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(du_data,du_labels)):            \n",
    "        feed_dict[dec_train_inputs[ui]] = dat\n",
    "        feed_dict[dec_train_labels[ui]] = lbl\n",
    "        feed_dict[dec_label_masks[ui]] = (np.array([ui for _ in range(batch_size)])<du_lengths).astype(np.int32)\n",
    "        \n",
    "        \n",
    "    _,l,tr_pred = sess.run([adam_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "    tr_pred = tr_pred.flatten()\n",
    "    \n",
    "    \n",
    "    if (i+1)%500==0:\n",
    "        rand_idx = np.random.randint(low=1,high=batch_size)\n",
    "        print_str = 'Actual: '\n",
    "        for w in np.concatenate(du_labels,axis=0)[rand_idx::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "\n",
    "            \n",
    "        print()\n",
    "        print_str = 'Predicted: '\n",
    "        for w in tr_pred[rand_idx::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "        print()        \n",
    "        \n",
    "    avg_loss += l\n",
    "    \n",
    "    if (i+1)%500==0:\n",
    "      print('============= Step ', str(i+1), ' =============')\n",
    "      print('\\t Loss: ',avg_loss/500.0)\n",
    "      #print(avg_loss)\n",
    "        \n",
    "      loss_over_time.append(avg_loss/500.0)\n",
    "             \n",
    "      avg_loss = 0.0\n",
    "      sess.run(inc_gstep)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "ak9MM1TUGHm5",
    "outputId": "a964cd45-a3d1-4502-ba13-24554c2a220f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: is booted <unk> , it will mount the <unk> as its root filesystem <unk> . </s> \n",
      "\n",
      "Predicted: <unk> a <unk> , and is be the ability <unk> a name <unk> <unk> . </s> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.random.randint(low=1,high=batch_size)\n",
    "print_str = 'Actual: '\n",
    "for w in np.concatenate(du_labels,axis=0)[rand_idx::batch_size].tolist():\n",
    "    print_str += tgt_reverse_dictionary[w] + ' '\n",
    "    if tgt_reverse_dictionary[w] == '</s>':\n",
    "        break\n",
    "print(print_str)\n",
    "\n",
    "\n",
    "print()\n",
    "print_str = 'Predicted: '\n",
    "for w in tr_pred[rand_idx::batch_size].tolist():\n",
    "    print_str += tgt_reverse_dictionary[w] + ' '\n",
    "    if tgt_reverse_dictionary[w] == '</s>':\n",
    "        break\n",
    "print(print_str)\n",
    "print()  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NMT.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
